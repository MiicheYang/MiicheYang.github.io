<!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Zeyuan Yang</title>

    <meta name="author" content="Zeyuan Yang">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="shortcut icon" href="images/favicon/bear.ico" type="image/x-icon">
    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    
  </head>

  <body>
    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr style="padding:0px">
              <td style="padding:2.5%;width:63%;vertical-align:middle">
                <p class="name" style="text-align: center;">
                  Zeyuan Yang
                </p>
                <p>I am a first year PhD student at <a href="=https://www.umass.edu/">University of Massachusetts Amherst</a>, working under the supervision of <a href="https://people.csail.mit.edu/ganchuang/">Prof. Chuang Gan</a> and <a href="https://hcr.cs.umass.edu/index.html">Prof. Hao Zhang</a>.
                </p>
                <p>
                  Zeyuan received his master's degree in Computer Science at <a href="=https://www.tsinghua.edu.cn/en/">Tsinghua University</a>. During his master study, he was fortunate to be mentored by <a href="https://nlp.csai.tsinghua.edu.cn/~ly/">Prof. Yang Liu</a> and <a href="http://www.lpeng.net/">Prof. Peng Li</a>. Before graduate study, he received his bachelor's degree from the School of Economics and Management at Tsinghua University.
                </p>
                <p style="text-align:center">
                  <a href="yangzeyuan2020@gmail.com">Email</a> &nbsp;/&nbsp;
                  <a href="data/cvs/CV-2024-12.pdf">CV</a> &nbsp;/&nbsp;
                  <!-- <a href="data/Zeyuan-bio.txt">Bio</a> &nbsp;/&nbsp; -->
                  <a href="https://scholar.google.com/citations?user=k_qpTh4AAAAJ&hl=en">Scholar</a> &nbsp;/&nbsp;
                  <a href="https://x.com/miiche_yang">Twitter</a> &nbsp;/&nbsp;
                  <a href="https://github.com/MiicheYang">Github</a>
                </p>
              </td>
              <td style="padding:2.5%;width:37%;max-width:37%">
                <a href="images/profile/selfie-sg.jpg"><img style="width:100%;max-width:100%;object-fit: cover; border-radius: 50%;" alt="profile photo" src="images/profile/selfie-sg.jpg" class="hoverZoomLink"></a>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
              <td style="padding:16px;width:100%;vertical-align:middle">
                <h2>Research</h2>
                <p>
                  I am broadly interested in <b>embodied intelligence</b> and <b>multi-modal foundation models</b>. Currently, I focus on <b>building lifelong embodied agents executable in real-world environments</b>. I welcome collaboration opportunities and encourages interested individuals to reach out.
                </p>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      
      <tr>
        <td style="padding:16px;width:20%;vertical-align:middle">
          <img src="images/VCA.jpg" alt="clean-usnob" width="160" height="160">
        </td>
        <td style="padding:8px;width:80%;vertical-align:middle">
          <a href="https://vis-www.cs.umass.edu/vca-website/">
        <span class="papertitle">VCA: Video Curious Agent for Long Video Understanding</span>
          </a>
          <br>
            <strong>Zeyuan Yang</strong><sup>‚Ä°</sup>,
            <a href="https://chendl02.github.io/">Delin Chen<sup>‚Ä°</sup></a>,
            <a href="https://openreview.net/profile?id=~Xueyang_Yu1">Xueyang Yu</a>,
            <a href="https://maohaos2.github.io/Maohao/">Maohao Shen</a>,
            <a href="https://people.csail.mit.edu/ganchuang/">Chuang Gan</a>
          <br>
          <em>arXiv</em>, 2024
          <br>
          <a href="https://vis-www.cs.umass.edu/vca-website/">Project</a>
          |
          <a href="https://www.arxiv.org/abs/2412.10471">Paper</a>
          <p></p>
          <p>
            In this work, we introduce VCA, a curiosity-driven video agent with self-exploration capability, which autonomously navigates video segments and efficiently builds a comprehensive understanding of complex video sequences.
          </p>
        </td>
      </tr>

      <tr>
        <td style="padding:16px;width:20%;vertical-align:middle">
          <img src="images/LCCL.jpg" alt="clean-usnob" width="160" height="80">
        </td>
        <td style="padding:8px;width:80%;vertical-align:middle">
          <a href="https://coling2025.org/program/main_conference_papers/">
            <span class="papertitle">Rethinking Long Context Generation from the Continual Learning Perspective</span>
          </a>
          <br>
            <strong>Zeyuan Yang</strong>, 
            <a href="https://openreview.net/profile?id=~Fangzhou_Xiong1">Fangzhou Xiong</a>, 
            <a href="https://lpeng.net/">Peng Li</a>, 
            <a href="https://nlp.csai.tsinghua.edu.cn/~ly/">Yang Liu</a>
          <br>
          <em>COLING</em>, 2025
          <p>In this paper, we inspect existing representative approaches and analyze their synergy with continual learning strategies. Also, we integrate these strategies into current approaches to further boost LLMs' efficiency in processing long contexts.</p>
        </td>
      </tr>

      <tr>
        <td style="padding:20px;width:20%;vertical-align:middle">
          <a href="https://vis-www.cs.umass.edu/ubsoft/">
            <video playsinline autoplay loop muted src="images/ubsoft1.mp4" alt="sym" width="70" style="padding-top:0px;padding-bottom:0px;border-radius:15px;"></video></a>
            <video playsinline autoplay loop muted src="images/ubsoft2.mp4" alt="sym" width="70" style="padding-top:0px;padding-bottom:0px;border-radius:15px;"></video></a>
            <video playsinline autoplay loop muted src="images/ubsoft3.mp4" alt="sym" width="70" style="padding-top:0px;padding-bottom:0px;border-radius:15px;"></video></a>
            <video playsinline autoplay loop muted src="images/ubsoft4.mp4" alt="sym" width="70" style="padding-top:0px;padding-bottom:0px;border-radius:15px;"></video></a>
            <video playsinline autoplay loop muted src="images/ubsoft5.mp4" alt="sym" width="70" style="padding-top:0px;padding-bottom:0px;border-radius:15px;"></video></a>
            <video playsinline autoplay loop muted src="images/ubsoft6.mp4" alt="sym" width="70" style="padding-top:0px;padding-bottom:0px;border-radius:15px;"></video></a>
            <video playsinline autoplay loop muted src="images/ubsoft7.mp4" alt="sym" width="70" style="padding-top:0px;padding-bottom:0px;border-radius:15px;"></video></a>
            <video playsinline autoplay loop muted src="images/ubsoft8.mp4" alt="sym" width="70" style="padding-top:0px;padding-bottom:0px;border-radius:15px;"></video></a>
          </a>
      </td>
        <td style="padding:8px;width:80%;vertical-align:middle">
          <a href="https://vis-www.cs.umass.edu/ubsoft/">
            <span class="papertitle">UBSoft: A Simulation Platform for Robotic Skill Learning in Unbounded Soft Environments</span>
          </a>
          <br>
            <a href="https://chunru-lin.github.io/">Chunru Lin<sup>‚Ä°</sup></a>,
            <a href="https://github.com/felixfjg">Jungang Fan<sup>‚Ä°</sup></a>,
            <a href="https://wangyian-me.github.io/">Yian Wang</a>,
            <strong>Zeyuan Yang</strong>, 
            <a href="https://acmlczh.github.io/">Zhehuan Chen</a>, 
            <a href="https://owenowl.github.io/">Lixin Fang</a>, 
            <a href="https://zswang666.github.io/">Tsun-Hsuan Wang</a>, 
            <a href="https://www.zhou-xian.com/">Xian Zhou</a>, 
            <a href="https://people.csail.mit.edu/ganchuang/">Chuang Gan</a>
          <br>
          <em>CoRL</em>, 2024
          <br>
          <a href="https://vis-www.cs.umass.edu/ubsoft/">Project</a>
          |
          <a href="https://arxiv.org/abs/2411.12711">Paper</a>
          |
          <a href="https://openreview.net/forum?id=7vzDBvviRO&referrer=%5BAuthor%20Console%5D(%2Fgroup%3Fid%3Drobot-learning.org%2FCoRL%2F2024%2FConference%2FAuthors%23your-submissions)">Code</a>
          <p></p>
          <br>
          <p>In this paper, we introduce UBSOFT, a new simulation platform designed to support unbounded soft environments for robot skill acquisition.</p>
        </td>
      </tr>

      <tr>
        <td style="padding:16px;width:20%;vertical-align:middle">
          <img src="images/UA2.jpg" alt="clean-usnob" width="160" height="100">
        </td>
        <td style="padding:8px;width:80%;vertical-align:middle">
          <a href="https://arxiv.org/abs/2402.07744">
            <span class="papertitle">Towards Unified Alignment Between Agents, Humans, and Environment</span>
          </a>
          <br>
            <a href="https://minicheshire.github.io/">Zonghan Yang<sup>‚Ä°</sup></a>, 
            <a href="https://scholar.google.com/citations?user=uckSAI0AAAAJ&hl=en">An Liu<sup>‚Ä°</sup></a>, 
            <a href="https://scholar.google.com/citations?user=vXsVhPcAAAAJ">Zijun Liu<sup>‚Ä°</sup></a>, 
            <a href="https://openreview.net/profile?id=~Kaiming_Liu1">Kaiming Liu</a>, 
            <a href="https://openreview.net/profile?id=~Fangzhou_Xiong1">Fangzhou Xiong</a>, 
            <a href="https://ylwangy.github.io/">Yile Wang</a>, 
            <strong>Zeyuan Yang</strong>, 
            <a href="https://openreview.net/profile?id=~Qingyuan_Hu3">Qingyuan Hu</a>, 
            <a href="https://openreview.net/profile?id=~XinRui_Chen2">Xinrui Chen</a>, 
            <a href="https://github.com/zzh2021010869">Zhenhe Zhang</a>, 
            <a href="https://scholar.google.com.hk/citations?user=AIKlZXcAAAAJ&hl=sv">Fuwen Luo</a>, 
            <a href="https://zhichengg.github.io/">Zhicheng Guo</a>, 
            <a href="https://lpeng.net/">Peng Li</a>, 
            <a href="https://nlp.csai.tsinghua.edu.cn/~ly/">Yang Liu</a>
          <br>
          <em>ICML</em>, 2024
          <br>
          <a href="https://agent-force.github.io/unified-alignment-for-agents.html">Project</a>
          |
          <a href="https://arxiv.org/abs/2402.07744">Paper</a>
          |
          <a href="https://github.com/AgentForceTeamOfficial/UA2-Agent">Code</a>
          <p></p>
          <br>
          <p>In this work, we introduce the principles of Unified Alignment for Agents (UA<sup>2</sup>), which advocate for the simultaneous alignment of agents with human intentions, environmental dynamics, and self-constraints such as the limitation of monetary budgets.</p>
        </td>
      </tr>

      <tr>
        <td style="padding:16px;width:20%;vertical-align:middle">
          <img src="images/RILA.jpg" alt="clean-usnob" width="160" height="100">
        </td>
        <td style="padding:8px;width:80%;vertical-align:middle">
          <a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Yang_RILA_Reflective_and_Imaginative_Language_Agent_for_Zero-Shot_Semantic_Audio-Visual_CVPR_2024_paper.pdf">
            <span class="papertitle">RILA: Reflective and Imaginative Language Agent for Zero-Shot Semantic Audio-Visual Navigation</span>
          </a>
          <br>
            <strong>Zeyuan Yang</strong><sup>‚Ä°</sup>, 
            <a href="https://jiagengliu02.github.io/">Jiageng Liu<sup>‚Ä°</sup></a>, 
            <a href="https://peihaochen.github.io/">Peihao Chen</a>, 
            <a href="https://users.cecs.anu.edu.au/~cherian/">Anoop Cherian</a>, 
            <a href="https://www.merl.com/people/tmarks">Tim K. Marks</a>, 
            <a href="https://www.jonathanleroux.org/">Jonathan Le Roux</a>, 
            <a href="https://people.csail.mit.edu/ganchuang/">Chuang Gan</a>
          <br>
          <em>CVPR</em>, 2024
          <br>
          <a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Yang_RILA_Reflective_and_Imaginative_Language_Agent_for_Zero-Shot_Semantic_Audio-Visual_CVPR_2024_paper.pdf">Paper</a>
          <p></p>
          <br>
          <p>In this work, we propose RILA, a reflective and imaginative agent for zero-shot semantic audio-visual navigation.</p>
        </td>
      </tr>

      <!-- <tr onmouseout="vca_stop()" onmouseover="vca_start()">
        <td style="padding:16px;width:20%;vertical-align:middle">
          <div class="one">
            <div class="two" id='cat4d_image'><video  width=100% height=100% muted autoplay loop>
            <source src="images/vca.mp4" type="video/mp4">
            Your browser does not support the video tag.
            </video></div>
            <img src='images/cat4d.jpg' width="160">
          </div>
          <script type="text/javascript">
            function vca_start() {
              document.getElementById('cat4d_image').style.opacity = "1";
            }

            function vca_stop() {
              document.getElementById('cat4d_image').style.opacity = "0";
            }
            cat4d_stop()
          </script>
        </td>
        <td style="padding:8px;width:80%;vertical-align:middle">
          <a href="https://vis-www.cs.umass.edu/vca-website/">
        <span class="papertitle">VCA: Video Curious Agent for Long Video Understanding</span>
          </a>
          <br>
            <strong>Zeyuan Yang</strong><sup>‚Ä°</sup>,
            <a href="https://chendl02.github.io/">Delin Chen<sup>‚Ä°</sup></a>,
            <a href="https://openreview.net/profile?id=~Xueyang_Yu1">Xueyang Yu</a>,
            <a href="https://maohaos2.github.io/Maohao/">Maohao Shen</a>,
            <a href="https://people.csail.mit.edu/ganchuang/">Chuang Gan</a>
          <br>
          <em>arXiv</em>, 2024
          <br>
          <a href="https://vis-www.cs.umass.edu/vca-website/">project page</a>
          /
          <a href="https://www.arxiv.org/abs/2412.10471">arXiv</a>
          <p></p>
          <p>
            In this work, we introduce VCA, a curiosity-driven video agent with self-exploration capability, which autonomously navigates video segments and efficiently builds a comprehensive understanding of complex video sequences.
          </p>
        </td>
      </tr> -->

      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
        <tr>
          <td style="padding:0px">
            <h2><b>Internship</b></h2>
          </td>
        </tr>
      </tbody></table>

      <table width="100%" align="center" border="0" cellpadding="20"><tbody>  
        <tr>
          <td width="100%" valign="center" style="padding:0px">
            <ul>
              <li>
                <span class="papertitle">Zhiyuan Innovation Technology - Research Intern <em>(Jan. 2024 - Aug. 2024)</em></span>
                <br>
                Manager: <a href="https://zsdonghao.github.io/">Hao Dong</a>
              </li>
              <li>
                <span class="papertitle">Ruilai Wisdom Technology - Research Intern <em>(Jan. 2021 - Jun. 2021)</em></span>
                <br>
                Manager: <a href="https://scholar.google.com/citations?user=iv1VFmsAAAAJ&hl=en">Shizhen Xu</a>
              <li>
                <span class="papertitle">Microsoft Research Asia - Research Intern <em>(Jun. 2020 - Dec. 2020)</em></span>
                <br>
                Manager: <a href="https://www.microsoft.com/en-us/research/people/hul/">Lin Huang</a>
              </li>
              <li>
                <span class="papertitle">Natural Language Processing Group, JD AI Lab - Research Intern <em>(Jan. 2020 - Jun. 2020)</em></span>
                <br>
                Manager: <a href="https://chenmengdx.github.io/">Prof. Meng Chen</a>
              </li>
            </ul>
          </td>
        </tr>      
      </tbody></table>

      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
        <tr>
          <td style="padding:0px">
            <h2><b>Service</b></h2>
          </td>
        </tr>
      </tbody></table>

      <table width="100%" align="center" border="0" cellpadding="20"><tbody>  
        <tr>
          <td width="100%" valign="center" style="padding:0px">
            <ul>
              <li>
                <em>Conference Reviewer:</em> 
                <br>
                ARR 2024, ICML 2025
              </li>
              <li>
                <em>Teaching Assistant:</em>
                <br>
                <span class="papertitle">Objected Oriented Programming, University of Massachusetts, Amherst <em>(Fall 2024)</em> </span>
                <br>
                <span class="papertitle">Foundations of Programming, University of Massachusetts, Amherst <em>(Spring 2025)</em> </span>
            </ul>
          </td>
        </tr>      
      </tbody></table>


      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
        <tr>
          <td style="padding:0px; text-align: center;">
            üèÄ ü•Å üé¨ üç≥ ‚úàÔ∏è üì∑ üìñ
            <!-- <span style="font-size: 10px">üíñ</span> -->
          </td>
        </tr>
      </tbody></table>
            
        </td>
      </tr>
    </table>
  </body>
</html>
