<!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Zeyuan Yang</title>

    <meta name="author" content="Zeyuan Yang">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="shortcut icon" href="images/favicon/bear.ico" type="image/x-icon">
    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    
  </head>

  <body>
    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr style="padding:0px">
              <td style="padding:2.5%;width:72%;vertical-align:middle">
                <p class="name" style="text-align: center;">
                  Zeyuan Yang
                </p>
                <p>I am a first year PhD student at <a href="=https://www.umass.edu/">University of Massachusetts Amherst</a>, working under the supervision of <a href="https://people.csail.mit.edu/ganchuang/">Prof. Chuang Gan</a> and <a href="https://hcr.cs.umass.edu/index.html">Prof. Hao Zhang</a>.
                </p>
                <p>
                  I received my master's degree in Computer Science at <a href="=https://www.tsinghua.edu.cn/en/">Tsinghua University</a>. During my master study, I was fortunate to be mentored by <a href="https://nlp.csai.tsinghua.edu.cn/~ly/">Prof. Yang Liu</a> and <a href="http://www.lpeng.net/">Prof. Peng Li</a>. Before graduate study, I received my bachelor's degree from the School of Economics and Management at Tsinghua University.
                </p>
                <p style="text-align:center">
                  <a href="yangzeyuan2020@gmail.com">Email</a> &nbsp;/&nbsp;
                  <a href="data/cvs/CV-2024-12.pdf">CV</a> &nbsp;/&nbsp;
                  <!-- <a href="data/Zeyuan-bio.txt">Bio</a> &nbsp;/&nbsp; -->
                  <a href="https://scholar.google.com/citations?user=k_qpTh4AAAAJ&hl=en">Scholar</a> &nbsp;/&nbsp;
                  <a href="https://x.com/miiche_yang">Twitter</a> &nbsp;/&nbsp;
                  <a href="https://github.com/MiicheYang">Github</a>
                </p>
              </td>
              <td style="padding:2.5%;width:28%;max-width:28%">
                <a href="images/profile/selfie-sg.jpg"><img style="width:100%;max-width:100%;object-fit: cover; border-radius: 50%;" alt="profile photo" src="images/profile/selfie-sg.jpg" class="hoverZoomLink"></a>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:16px;width:100%;vertical-align:middle">
                <h2>News</h2>
                <ul>
                  <li>2025.06: One paper <a href="https://www.arxiv.org/abs/2412.10471">VCA</a> is accepted by <a href="https://iccv.thecvf.com/Conferences/2025">ICCV 2025</a>.</li>
                  <li>2025.06: Introduce <a href="https://vlm-mirage.github.io/">Mirage</a>.</li>
                  <li>2025.05: Internship at <a href="https://sra.samsung.com/">Samsung</a> this summer, at Mountain View, California.</li>
                  <li>2024.09: Starting my PhD Journey at UMass Amherst.</li>
                </ul>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:16px;width:100%;vertical-align:middle">
                <h2>Research</h2>
                <p>
                  I am broadly interested in <b>embodied intelligence</b> and <b>multi-modal foundation models</b>. Currently, I focus on <b>building lifelong embodied agents executable in real-world environments</b>. I welcome collaboration opportunities and encourages interested individuals to reach out.
                </p>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      
      <tr>
        <td style="padding:16px;width:25%;vertical-align:middle">
          <video width="160" height="90" autoplay loop muted playsinline>
            <source src="images/mirage.mp4" type="video/mp4">
            Your browser does not support the video tag.
          </video>
        </td>
        <td style="padding:8px;width:75%;vertical-align:middle">
          <a href="https://vis-www.cs.umass.edu/vca-website/">
        <span class="papertitle">Machine Mental Imagery: Empower Multimodal Reasoning with Latent Visual Tokens</span>
          </a>
          <br>
            <strong>Zeyuan Yang</strong><sup>‡</sup>,
            <a href="https://openreview.net/profile?id=~Xueyang_Yu1">Xueyang Yu<sup>‡</sup></a>,
            <a href="https://chendl02.github.io/">Delin Chen</a>,
            <a href="https://maohaos2.github.io/Maohao/">Maohao Shen</a>,
            <a href="https://people.csail.mit.edu/ganchuang/">Chuang Gan</a>
          <br>
          <em>arXiv</em>, 2025
          <br>
          <a href="https://vlm-mirage.github.io/">Project</a>
          |
          <a href="https://www.arxiv.org/abs/2506.17218">Paper</a>
          |
          <a href="https://github.com/UMass-Embodied-AGI/Mirage">Code</a>
          <p></p>
          <p>
          We propose Mirage, interleaving latent visual tokens, which represent compact imagery visual features, with explicit text tokens to solve diverse multimodal reasoning tasks, boosting the reasoning performance without the full pixel-level image generation.
          </p>
        </td>
      </tr>

      <tr>
        <td style="padding:16px;width:25%;vertical-align:middle">
          <img src="images/VCA.jpg" alt="clean-usnob" width="160" height="160">
        </td>
        <td style="padding:8px;width:75%;vertical-align:middle">
          <a href="https://vis-www.cs.umass.edu/vca-website/">
        <span class="papertitle">VCA: Video Curious Agent for Long Video Understanding</span>
          </a>
          <br>
            <strong>Zeyuan Yang</strong><sup>‡</sup>,
            <a href="https://chendl02.github.io/">Delin Chen<sup>‡</sup></a>,
            <a href="https://openreview.net/profile?id=~Xueyang_Yu1">Xueyang Yu</a>,
            <a href="https://maohaos2.github.io/Maohao/">Maohao Shen</a>,
            <a href="https://people.csail.mit.edu/ganchuang/">Chuang Gan</a>
          <br>
          <em>ICCV</em>, 2025
          <br>
          <a href="https://vis-www.cs.umass.edu/vca-website/">Project</a>
          |
          <a href="https://www.arxiv.org/abs/2412.10471">Paper</a>
          <p></p>
          <p>
            In this work, we introduce VCA, a curiosity-driven video agent with self-exploration capability, which autonomously navigates video segments and efficiently builds a comprehensive understanding of complex video sequences.
          </p>
        </td>
      </tr>

      <tr>
        <td style="padding:16px;width:25%;vertical-align:middle">
          <img src="images/LCCL.jpg" alt="clean-usnob" width="160" height="80">
        </td>
        <td style="padding:8px;width:75%;vertical-align:middle">
          <a href="https://coling2025.org/program/main_conference_papers/">
            <span class="papertitle">Rethinking Long Context Generation from the Continual Learning Perspective</span>
          </a>
          <br>
            <strong>Zeyuan Yang</strong>, 
            <a href="https://openreview.net/profile?id=~Fangzhou_Xiong1">Fangzhou Xiong</a>, 
            <a href="https://lpeng.net/">Peng Li</a>, 
            <a href="https://nlp.csai.tsinghua.edu.cn/~ly/">Yang Liu</a>
          <br>
          <em>COLING</em>, 2025
          <p>In this paper, we inspect existing representative approaches and analyze their synergy with continual learning strategies. Also, we integrate these strategies into current approaches to further boost LLMs' efficiency in processing long contexts.</p>
        </td>
      </tr>

      <tr>
        <td style="padding:20px;width:25%;vertical-align:middle">
          <a href="https://vis-www.cs.umass.edu/ubsoft/">
            <video playsinline autoplay loop muted src="images/ubsoft1.mp4" alt="sym" width="70" style="padding-top:0px;padding-bottom:0px;border-radius:15px;"></video></a>
            <!-- <video playsinline autoplay loop muted src="images/ubsoft2.mp4" alt="sym" width="70" style="padding-top:0px;padding-bottom:0px;border-radius:15px;"></video></a> -->
            <video playsinline autoplay loop muted src="images/ubsoft3.mp4" alt="sym" width="70" style="padding-top:0px;padding-bottom:0px;border-radius:15px;"></video></a>
            <!-- <video playsinline autoplay loop muted src="images/ubsoft4.mp4" alt="sym" width="70" style="padding-top:0px;padding-bottom:0px;border-radius:15px;"></video></a> -->
            <video playsinline autoplay loop muted src="images/ubsoft5.mp4" alt="sym" width="70" style="padding-top:0px;padding-bottom:0px;border-radius:15px;"></video></a>
            <!-- <video playsinline autoplay loop muted src="images/ubsoft6.mp4" alt="sym" width="70" style="padding-top:0px;padding-bottom:0px;border-radius:15px;"></video></a> -->
            <video playsinline autoplay loop muted src="images/ubsoft7.mp4" alt="sym" width="70" style="padding-top:0px;padding-bottom:0px;border-radius:15px;"></video></a>
            <!-- <video playsinline autoplay loop muted src="images/ubsoft8.mp4" alt="sym" width="70" style="padding-top:0px;padding-bottom:0px;border-radius:15px;"></video></a> -->
          </a>
      </td>
        <td style="padding:8px;width:75%;vertical-align:middle">
          <a href="https://vis-www.cs.umass.edu/ubsoft/">
            <span class="papertitle">UBSoft: A Simulation Platform for Robotic Skill Learning in Unbounded Soft Environments</span>
          </a>
          <br>
            <a href="https://chunru-lin.github.io/">Chunru Lin<sup>‡</sup></a>,
            <a href="https://github.com/felixfjg">Jungang Fan<sup>‡</sup></a>,
            <a href="https://wangyian-me.github.io/">Yian Wang</a>,
            <strong>Zeyuan Yang</strong>, 
            <a href="https://acmlczh.github.io/">Zhehuan Chen</a>, 
            <a href="https://owenowl.github.io/">Lixin Fang</a>, 
            <a href="https://zswang666.github.io/">Tsun-Hsuan Wang</a>, 
            <a href="https://www.zhou-xian.com/">Xian Zhou</a>, 
            <a href="https://people.csail.mit.edu/ganchuang/">Chuang Gan</a>
          <br>
          <em>CoRL</em>, 2024
          <br>
          <a href="https://vis-www.cs.umass.edu/ubsoft/">Project</a>
          |
          <a href="https://arxiv.org/abs/2411.12711">Paper</a>
          |
          <a href="https://openreview.net/forum?id=7vzDBvviRO&referrer=%5BAuthor%20Console%5D(%2Fgroup%3Fid%3Drobot-learning.org%2FCoRL%2F2024%2FConference%2FAuthors%23your-submissions)">Code</a>
          <p></p>
          <p>In this paper, we introduce UBSOFT, a new simulation platform designed to support unbounded soft environments for robot skill acquisition.</p>
        </td>
      </tr>

      <tr>
        <td style="padding:16px;width:25%;vertical-align:middle">
          <img src="images/UA2.jpg" alt="clean-usnob" width="160" height="100">
        </td>
        <td style="padding:8px;width:75%;vertical-align:middle">
          <a href="https://arxiv.org/abs/2402.07744">
            <span class="papertitle">Towards Unified Alignment Between Agents, Humans, and Environment</span>
          </a>
          <br>
            <a href="https://minicheshire.github.io/">Zonghan Yang<sup>‡</sup></a>, 
            <a href="https://scholar.google.com/citations?user=uckSAI0AAAAJ&hl=en">An Liu<sup>‡</sup></a>, 
            <a href="https://scholar.google.com/citations?user=vXsVhPcAAAAJ">Zijun Liu<sup>‡</sup></a>, 
            <a href="https://openreview.net/profile?id=~Kaiming_Liu1">Kaiming Liu</a>, 
            <a href="https://openreview.net/profile?id=~Fangzhou_Xiong1">Fangzhou Xiong</a>, 
            <a href="https://ylwangy.github.io/">Yile Wang</a>, 
            <strong>Zeyuan Yang</strong>, 
            <a href="https://openreview.net/profile?id=~Qingyuan_Hu3">Qingyuan Hu</a>, 
            <a href="https://openreview.net/profile?id=~XinRui_Chen2">Xinrui Chen</a>, 
            <a href="https://github.com/zzh2021010869">Zhenhe Zhang</a>, 
            <a href="https://scholar.google.com.hk/citations?user=AIKlZXcAAAAJ&hl=sv">Fuwen Luo</a>, 
            <a href="https://zhichengg.github.io/">Zhicheng Guo</a>, 
            <a href="https://lpeng.net/">Peng Li</a>, 
            <a href="https://nlp.csai.tsinghua.edu.cn/~ly/">Yang Liu</a>
          <br>
          <em>ICML</em>, 2024
          <br>
          <a href="https://agent-force.github.io/unified-alignment-for-agents.html">Project</a>
          |
          <a href="https://arxiv.org/abs/2402.07744">Paper</a>
          |
          <a href="https://github.com/AgentForceTeamOfficial/UA2-Agent">Code</a>
          <p></p>
          <p>In this work, we introduce the principles of Unified Alignment for Agents (UA<sup>2</sup>), which advocate for the simultaneous alignment of agents with human intentions, environmental dynamics, and self-constraints such as the limitation of monetary budgets.</p>
        </td>
      </tr>

      <tr>
        <td style="padding:16px;width:25%;vertical-align:middle">
          <img src="images/RILA.jpg" alt="clean-usnob" width="160" height="100">
        </td>
        <td style="padding:8px;width:75%;vertical-align:middle">
          <a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Yang_RILA_Reflective_and_Imaginative_Language_Agent_for_Zero-Shot_Semantic_Audio-Visual_CVPR_2024_paper.pdf">
            <span class="papertitle">RILA: Reflective and Imaginative Language Agent for Zero-Shot Semantic Audio-Visual Navigation</span>
          </a>
          <br>
            <strong>Zeyuan Yang</strong><sup>‡</sup>, 
            <a href="https://jiagengliu02.github.io/">Jiageng Liu<sup>‡</sup></a>, 
            <a href="https://peihaochen.github.io/">Peihao Chen</a>, 
            <a href="https://users.cecs.anu.edu.au/~cherian/">Anoop Cherian</a>, 
            <a href="https://www.merl.com/people/tmarks">Tim K. Marks</a>, 
            <a href="https://www.jonathanleroux.org/">Jonathan Le Roux</a>, 
            <a href="https://people.csail.mit.edu/ganchuang/">Chuang Gan</a>
          <br>
          <em>CVPR</em>, 2024
          <br>
          <a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Yang_RILA_Reflective_and_Imaginative_Language_Agent_for_Zero-Shot_Semantic_Audio-Visual_CVPR_2024_paper.pdf">Paper</a>
          <p></p>
          <p>In this work, we propose RILA, a reflective and imaginative agent for zero-shot semantic audio-visual navigation.</p>
        </td>
      </tr>

      <tr>
        <td style="padding:16px;width:25%;vertical-align:middle">
          <img src="images/TRAN.jpg" alt="clean-usnob" width="160" height="120">
        </td>
        <td style="padding:8px;width:75%;vertical-align:middle">
          <a href="https://arxiv.org/abs/2310.15746">
            <span class="papertitle">Failures Pave the Way: Enhancing Large Language Models through Tuning-free Rule Accumulation</span>
          </a>
          <br>
            <strong>Zeyuan Yang</strong>, 
            <a href="https://lpeng.net/">Peng Li</a>, 
            <a href="https://nlp.csai.tsinghua.edu.cn/~ly/">Yang Liu</a>
          <br>
          <em>EMNLP</em>, 2023
          <br>
          <a href="https://arxiv.org/abs/2310.15746">Paper</a>
          |
          <a href="https://github.com/THUNLP-MT/TRAN">Code</a>
          <p></p>
          <p>In this work, we propose our Tuning-free Rule Accumulation (TRAN) framework, which guides LLMs in improving their performance by learning from previous mistakes.</p>
        </td>
      </tr>

      <tr>
        <td style="padding:16px;width:25%;vertical-align:middle">
          <img src="images/ROGO.jpg" alt="clean-usnob" width="160" height="100">
        </td>
        <td style="padding:8px;width:75%;vertical-align:middle">
          <a href="https://www.sciencedirect.com/science/article/pii/S2666651023000128">
            <span class="papertitle">Restricted orthogonal gradient projection for continual learning</span>
          </a>
          <br>
            <strong>Zeyuan Yang</strong>, 
            <a href="https://minicheshire.github.io/">Zonghan Yang</a>,
            Yichen Liu, 
            <a href="https://lpeng.net/">Peng Li</a>, 
            <a href="https://nlp.csai.tsinghua.edu.cn/~ly/">Yang Liu</a>
          <br>
          <em>AI Open</em>, 2023
          <br>
          <a href="https://www.sciencedirect.com/science/article/pii/S2666651023000128">Paper</a>
          |
          <a href="https://github.com/THUNLP-MT/ROGO">Code</a>
          <p></p>
          <p>In this work, we propose the Restricted Orthogonal Gradient prOjection (ROGO) framework. The basic idea is to adopt a restricted orthogonal constraint allowing parameters optimized in the direction oblique to the whole frozen space to facilitate forward knowledge transfer while consolidating previous knowledge.</p>
        </td>
      </tr>

      <tr>
        <td style="padding:16px;width:25%;vertical-align:middle">
          <img src="images/DMB.png" alt="clean-usnob" width="160" height="160">
        </td>
        <td style="padding:8px;width:75%;vertical-align:middle">
          <a href="https://ieeexplore.ieee.org/document/9729651">
            <span class="papertitle">Dynamic Multi-branch Layers for On-device Neural Machine Translation</span>
          </a>
          <br>
            <a href="https://scholar.google.com/citations?user=iDFjEO0AAAAJ&hl=en">Zhixing Tan</a>,
            <strong>Zeyuan Yang</strong>, 
            <a href="https://zmlarry.github.io/">Meng Zhang</a>,
            <a href="https://liuquncn.github.io/index_en.html">Qun Liu</a>,
            <a href="https://www.cs.tsinghua.edu.cn/csen/info/1312/4394.htm">Maosong Sun</a>, 
            <a href="https://nlp.csai.tsinghua.edu.cn/~ly/">Yang Liu</a>
          <br>
          <em>IEEE/ACM Transactions on Audio, Speech, and Language Processing</em>, 2022
          <br>
          <a href="https://ieeexplore.ieee.org/document/9729651">Paper</a>
          |
          <a href="https://github.com/THUNLP-MT/Transformer-DMB">Code</a>
          <p></p>
          <p>In this work, we propose to improve the performance of on-device NMT systems with dynamic multi-branch layers. Specifically, we design a layer-wise dynamic multi-branch network with only one branch activated during training and inference.</p>
        </td>
      </tr>

      <tr>
        <td style="padding:16px;width:25%;vertical-align:middle">
          <img src="images/GMD.png" alt="clean-usnob" width="160" height="80">
        </td>
        <td style="padding:8px;width:75%;vertical-align:middle">
          <a href="https://gmd.copernicus.org/articles/14/4641/2021/">
            <span class="papertitle">Exploring Deep Learning for Air Pollutant Emission Estimation</span>
          </a>
          <br>
            <a href="https://www.microsoft.com/en-us/research/people/hul/">Lin Huang<sup>‡</sup></a>,
            <a href="https://www.researchgate.net/profile/Song-Liu-27">Song Liu<sup>‡</sup></a>,
            <strong>Zeyuan Yang</strong>, 
            <a href="https://www.tsinghua.edu.cn/enven/info/1036/1535.htm">Jia Xing</a>,
            <a href="https://jialrs.github.io/home/">Jia Zhang</a>,
            <a href="https://sites.google.com/view/jiangbian">Jiang Bian</a>,
            <a href="https://www.linkedin.com/in/siwei-li-821340269/">Siwei Li</a>,
            <a href="https://scholar.google.com/citations?user=X3FHxroAAAAJ&hl=en">Shovan Kumar Sahu</a>, 
            <a href="https://www.tsinghua.edu.cn/enven/info/1036/1578.htm">Shuxiao Wang</a>, 
            <a href="https://scholar.google.com/citations?user=Nh832fgAAAAJ&hl=en">Tie-Yan Liu</a>
          <br>
          <em>Geoscientific Model Development</em>, 2021
          <br>
          <a href="https://gmd.copernicus.org/articles/14/4641/2021/">Paper</a>
          <p></p>
          <p>In this study, we proposed a novel method to model the dual relationship between an emission inventory and pollution concentrations for emission inventory estimation.</p>
        </td>
      </tr>

      <!-- <tr onmouseout="vca_stop()" onmouseover="vca_start()">
        <td style="padding:16px;width:20%;vertical-align:middle">
          <div class="one">
            <div class="two" id='cat4d_image'><video  width=100% height=100% muted autoplay loop>
            <source src="images/vca.mp4" type="video/mp4">
            Your browser does not support the video tag.
            </video></div>
            <img src='images/cat4d.jpg' width="160">
          </div>
          <script type="text/javascript">
            function vca_start() {
              document.getElementById('cat4d_image').style.opacity = "1";
            }

            function vca_stop() {
              document.getElementById('cat4d_image').style.opacity = "0";
            }
            cat4d_stop()
          </script>
        </td>
        <td style="padding:8px;width:80%;vertical-align:middle">
          <a href="https://vis-www.cs.umass.edu/vca-website/">
        <span class="papertitle">VCA: Video Curious Agent for Long Video Understanding</span>
          </a>
          <br>
            <strong>Zeyuan Yang</strong><sup>‡</sup>,
            <a href="https://chendl02.github.io/">Delin Chen<sup>‡</sup></a>,
            <a href="https://openreview.net/profile?id=~Xueyang_Yu1">Xueyang Yu</a>,
            <a href="https://maohaos2.github.io/Maohao/">Maohao Shen</a>,
            <a href="https://people.csail.mit.edu/ganchuang/">Chuang Gan</a>
          <br>
          <em>arXiv</em>, 2024
          <br>
          <a href="https://vis-www.cs.umass.edu/vca-website/">project page</a>
          /
          <a href="https://www.arxiv.org/abs/2412.10471">arXiv</a>
          <p></p>
          <p>
            In this work, we introduce VCA, a curiosity-driven video agent with self-exploration capability, which autonomously navigates video segments and efficiently builds a comprehensive understanding of complex video sequences.
          </p>
        </td>
      </tr> -->

      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
        <tr>
          <td style="padding:0px">
            <h2><b>Internship</b></h2>
          </td>
        </tr>
      </tbody></table>

      <table width="100%" align="center" border="0" cellpadding="20"><tbody>  
        <tr>
          <td width="100%" valign="center" style="padding:0px">
            <ul>
              <li>
                <span class="papertitle">Samsung Research America - Research Intern <em>(May. 2025 - Now)</em></span>
                <br>
                Manager: <a href="https://luoweizhou.github.io/">Luowei Zhou</a>
              </li>
              <li>
                <span class="papertitle">Zhiyuan Innovation Technology - Research Intern <em>(Jan. 2024 - Aug. 2024)</em></span>
                <br>
                Manager: <a href="https://zsdonghao.github.io/">Prof. Hao Dong</a>
              </li>
              <li>
                <span class="papertitle">Ruilai Wisdom Technology - Research Intern <em>(Jan. 2021 - Jun. 2021)</em></span>
                <br>
                Manager: <a href="https://scholar.google.com/citations?user=iv1VFmsAAAAJ&hl=en">Shizhen Xu</a>
              <li>
                <span class="papertitle">Microsoft Research Asia - Research Intern <em>(Jun. 2020 - Dec. 2020)</em></span>
                <br>
                Manager: <a href="https://www.microsoft.com/en-us/research/people/hul/">Lin Huang</a>, <a href="https://jialrs.github.io/home/">Jia Zhang</a>
              </li>
              <li>
                <span class="papertitle">Natural Language Processing Group, JD AI Lab - Research Intern <em>(Jan. 2020 - Jun. 2020)</em></span>
                <br>
                Manager: <a href="https://chenmengdx.github.io/">Meng Chen</a>
              </li>
            </ul>
          </td>
        </tr>      
      </tbody></table>

      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
        <tr>
          <td style="padding:0px">
            <h2><b>Service</b></h2>
          </td>
        </tr>
      </tbody></table>

      <!-- <table width="100%" align="center" border="0" cellpadding="20"><tbody>  
        <tr>
          <td width="100%" valign="center" style="padding:0px">
            <ul>
              <li>
                <em>Conference Reviewer:</em> 
                <br>
                ARR 2024, ICML 2025
              </li>
              <li>
                <em>Teaching Assistant:</em>
                <br>
                <span class="papertitle">Objected Oriented Programming, University of Massachusetts, Amherst <em>(Fall 2024)</em> </span>
                <br>
                <span class="papertitle">Foundations of Programming, University of Massachusetts, Amherst <em>(Spring 2025)</em> </span>
            </ul>
          </td>
        </tr>      
      </tbody></table> -->

      <table width="100%" align="center" border="0" cellpadding="20"><tbody>
        <tr>
          <td width="100%" valign="center" style="padding:0px">
            <ul>
              <li>
                <em><strong>Conference Reviewer:</strong></em><br>
                ARR 2024, ICML 2025, ICCV 2025, NeurIPS 2025
              </li>
    
              <li style="margin-top:1em;">
                <em><strong>Teaching Assistant:</strong></em>
                <ul style="list-style: none; margin:0; padding:0; margin-top:0.5em;">
                  <li style="margin-bottom:0.5em;">
                    <span class="papertitle"><strong>Object-Oriented Programming</strong>, <em>University of Massachusetts, Amherst (Fall 2024)</em></span>
                    <br>
                    Instructors: Prof. <a href="https://people.cs.umass.edu/~jaimedavila/?_gl=1*i34nnc*_gcl_au*NTY0MDQ2OTY1LjE3MzE3MzEyMzU.*_ga*MTM4MzQ1MTUuMTcyMzU2MTQzNQ..*_ga_21RLS0L7EB*MTczNDY0NzM2NC45OS4wLjE3MzQ2NDczNjQuMC4wLjA.">Jaime Davila</a>, <a href="https://www.cics.umass.edu/about/directory/cole-reilly">Cole Reilly</a>
                  </li>
                  <li>
                    <span class="papertitle"><strong>Foundations of Programming</strong>, <em>University of Massachusetts, Amherst (Spring 2025)</em></span>
                    <br>
                    Instructors: Prof.<a href="https://www.cics.umass.edu/about/directory/ghazaleh-parvini">Ghazaleh Parvini</a>, <a href="https://www.cics.umass.edu/about/directory/cole-reilly">Cole Reilly</a>
                  </li>
                </ul>
              </li>
            </ul>
          </td>
        </tr>
      </tbody></table>


      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
        <br>
        <tr>
          <td style="padding:0px; text-align: center;">
            🎬 🥃 🥁 ✈️ 🏀 📖 🍳 📷
            <!-- <span style="font-size: 10px">💖</span> -->
          </td>
        </tr>
      </tbody></table>
            
        </td>
      </tr>
    </table>
  </body>
</html>
